Work in progress...

# Demo

In this demo, we study image classification with [CIFAR100](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. [This image](./img/cifar100.jpg) is a sample of what the dataset looks like. The dataset has 100 classes containing 600 images each.

We will train a CNN model called [ResNet152](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.resnet152.html). This model has over 60M parameters to train.

In this demo, we want to monitor the GPU utilization. Let's first refresh our memory about it.

## Monitoring GPU Utilization
When running jobs on LUMI's GPUs, you want to make sure you use the given computational resources as efficiently as possible. Your project will be billed for the number of GPUs you allocate times the number of hours you use them. If you only utilize half of the GPUs computational power, you are still billed for the full GPU, resulting in wasted resources and money. In this chapter, we will show how to monitor and profile your jobs to ensure you are using the resources efficiently.

### Monitoring jobs with `rocm-smi`

The `rocm-smi` tool is a command-line utility that allows you to monitor the status of AMD GPUs on LUMI. If the cluster you are using has Nvidia GPUs (like Mahti, you have to use `nvidia-smi`). The output of the command look similar to the following:

![Image title](./img/rocm-smi.png)

To use `rocm-smi` you have to access the node that your GPU job is running. To do that, you need the
  `jobid` of the job which `squeue --me` shows. With the `jobid`, you can open an interactive parallel session with the following command:

```bash
srun --jobid <jobID> --interactive --pty /bin/bash
```
This will open a shell on the compute node where the job is running. We can now use the `rocm-smi` tool to monitor the GPU usage. The following command will show the GPU usage updated every second:

```bash
watch -n1 rocm-smi
```
You can combine the previous commands by 
```bash
srun --interactive --pty --jobid=<jobID> watch -n 1 rocm-smi
```

The `rocm-smi` tool shows multiple useful metrics such as GPU utilization, memory usage, temperature, and power usage. The most intuitive metrics might be GPU utilization and memory usage, they are however not accurate indicators whether the GPU is fully utilized as a kernel waiting idle for data shows in the driver as 100% GPU utilization. The best indicator is instead the drawn power. For a single GPU, a power usage of around 300W is a good indicator that the full GPU is being leveraged. 

See [GPU-accelerated machine learning](https://docs.csc.fi/support/tutorials/gpu-ml/) documentations on LUMI for more information.

## Task 1

In the first task, we will use one single GPU to train the model Starting with `train_cifar100.py` and familiarize yourself with the codebase.

You can run the training directly with the corresponding script listed above:

    sbatch  run_single_cifar.sh

As a reminder, you can check the status of your runs with the command:

    squeue --me

The output of the run will appear in a file named `slurm-RUN_ID.out`
where `RUN_ID` is the Slurm batch job id. You can check the last ten
lines of that file with the command:

    tail slurm-RUN_ID.out

Use `tail -f` if you want to continuously follow the progress of the
output. (Press Ctrl-C when you want to stop following the file.)

## Task 2

Repeat the experiment with the `train_ddp_cifar100.py` Which trains the model with 2 GPUs with the [Distributed Data Parallel](https://docs.pytorch.org/tutorials/intermediate/ddp_tutorial.html) technique.

You can run the training directly with the corresponding script listed above:

    sbatch  run_ddp_cifar100.sh


## Task 3

Repeat the experimet with `train_data_parallel.py` which trains the model with 2 GPUs with the [Data Parallel](https://docs.pytorch.org/docs/stable/generated/torch.nn.DataParallel.html) technique.

You can run the training directly with the corresponding script listed above:

    sbatch  run_data_parallel_cifar100.sh

## Questions:
1. For each task, look at the GPU utilization, and VRAM. Discuss on how to increase the VRAM.
2. By looking at the `slurm-RUN_ID.out`, look for the training speed for the each iteration and also one epoch. Why the first epoch in all tasks is slower than other epochs?
3. Why the total number of iterations are different in DDP?
4. Do you see any overheads for the DDP training?
5. Why using 2 GPUs in Data Parallel approach is not as fast as DDP? How much is the overhead?
6. In `train_cifar100.py`, change investigate the effect of switching to a smaller model of `resnet50`. How it affects the training speed? Does it match your expectations based on the number of parameters?
7. In `train_cifar100.py`, change investigate the effect of switching to a larger batch size. How it affects the training speed? Does it match your expectations based on VRAM utilization?

   
## Monitoring with visulatization

You can use TensorBoard either via the LUMI web user interface.

### Tensorboard via LUMI web interface

1. Log in via <https://www.lumi.csc.fi/>
2. Select menu item: Apps â†’ TensorBoard
4. In the form:
   - Select course project: project_462000956
   - Specify the "TensorBoard log directory", it's where you have cloned the course repository plus "hpc-ai/logs", for example:
  `/scratch/project_462000956/$USER/summerschool/hpc-ai/demo`. You can run `pwd` in the terminal to find out the full path where you are working.
   - Leave rest at default settings
6. Click "Launch"
7. Wait until you see the "Connect to Tensorboard" button, then click that.
8. When you're done using TensorBoard, please go to "My Interactive Sessions" in the LUMI web user interface and "Cancel" the session. (It will automatically terminate once the reserved time is up, but it's always better to release the resource as soon as possible so that others can use it.)

### PyTorch profiler

Using `rocm-smi` can give us an easy way to peek at GPU utilization, but it doesn't provide any information about which parts of the code are taking the most time. For this, we can use PyTorch's built-in profiler. We create the profile in the training scripts, and save the required data by calling `profiler.step()`. 

The output of the profiling will be saved in a `trace.json` file located in `./logs/profiler`. We can visualize the trace using the Chrome browser by copying the `trace.json` file to our local machine. If you are using Google Chrome, you can navigate to `chrome://tracing` or use the Perfetto webite by navigating to [ui.perfetto.dev/](https://ui.perfetto.dev/) and uploading the `trace.json` file. The trace will show us the time spent in each function call, and will look similar to the following:

![Image title](./img/perfetto-trace.png)

If the framework-level profiling is not sufficient and you want to investigate hardware-level performance, you can use AMD's profiling tools. The [LUMI training materials](https://lumi-supercomputer.github.io/LUMI-training-materials/) has more details information.
